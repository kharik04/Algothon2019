{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#created by Khariton Gorbunov as a part of BlackRock X Imperial College Algothon 2019\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define network\n",
    "network=[0,4,3,3]#hidden neurons, 0 for 0th input\n",
    "added_input=[3,2,2,0]#added input, zero for output layer only\n",
    "activation_def=[\"id\",\"id\",\"id\"]#activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#activation function method\n",
    "def sigma(x,act_type):\n",
    "    if(act_type==\"id\"):\n",
    "        return(x)\n",
    "    else:\n",
    "        return(x)\n",
    "#initialise parameters    \n",
    "def init_weights(weights, network,added_input):\n",
    "    for layer in range(1,len(network)):\n",
    "        weight=np.ones((network[layer],network[layer-1]+added_input[layer-1]))\n",
    "        weights.append(weight)\n",
    "def init_biases(biases, network):\n",
    "    for layer in range(1,len(network)):\n",
    "        bias=np.ones(network[layer])\n",
    "        biases.append(bias)\n",
    "#linear algebra loop for feed forward\n",
    "def feed_forward(input_vec, weights, biases, activation_def):#input vec format: [x0,x1,x2,x3,..] where xn is vector input at layer n\n",
    "    temp=input_vec[0]\n",
    "    for i,(weight, bias, activation) in enumerate(zip(weights, biases, activation_def),1):\n",
    "        temp=np.matmul(weight, temp)\n",
    "        temp=np.add(temp, bias)\n",
    "        temp=sigma(temp,activation)\n",
    "        if(i<len(weights)):#avoid last layer, no input there. len(weights) is that size\n",
    "            temp=np.concatenate((temp, input_vec[i]))#add feature into hidden layers\n",
    "    return temp\n",
    "#numerical differentiation based gradient descent\n",
    "def grad_descent(input_vec, weights, biases, activation_def, label, lr):\n",
    "    #consider storing initial weights\n",
    "    weightsIT=weights.copy()\n",
    "    biasesIT=biases.copy()\n",
    "    feedForward=feed_forward(input_vec[0], weights, biases, activation_def)#inititalise\n",
    "    dh=0.1\n",
    "    for k, weight in enumerate(weightsIT):#update weights\n",
    "        for i in range(len(weight)):\n",
    "            for j in range(len(weight[0])):\n",
    "                grad=0\n",
    "                for input_feature, label_feature in zip(input_vec,label):\n",
    "                    feedForward=feed_forward(input_feature, weightsIT, biasesIT, activation_def)#can optimize\n",
    "                    weightsCopy=weightsIT.copy()\n",
    "                    diff=np.add(feedForward,-1*label_feature)\n",
    "                    weightsCopy[k][i][j]+=dh#consider just subtracting after\n",
    "                    y2=feed_forward(input_feature, weightsCopy, biasesIT, activation_def)\n",
    "                    weightsCopy[k][i][j]-=dh\n",
    "                    deltaY=np.add(y2,-1*feedForward)\n",
    "                    dydw=deltaY/dh\n",
    "                    gradcomp=np.dot(diff, dydw)\n",
    "                    grad=np.add(grad, gradcomp)\n",
    "                weights[k][i][j]=weights[k][i][j]-lr*grad/len(input_vec)\n",
    "    for k, bias in enumerate(biasesIT):#update biases\n",
    "        for i in range(len(bias)):\n",
    "            grad=0\n",
    "            for input_feature, label_feature in zip(input_vec,label):\n",
    "                feedForward=feed_forward(input_feature, weightsIT, biasesIT, activation_def)\n",
    "                biasesCopy=biasesIT.copy()\n",
    "                diff=np.add(feedForward,-1*label_feature)\n",
    "                biasesCopy[k][i]+=dh#consider just subtracting after\n",
    "                y2=feed_forward(input_feature, weightsIT, biasesCopy, activation_def)\n",
    "                biasesCopy[k][i]-=dh\n",
    "                deltaY=np.add(y2,-1*feedForward)\n",
    "                dydb=deltaY/dh\n",
    "                gradcomp=np.dot(diff, dydb)\n",
    "                grad=np.add(grad, gradcomp)\n",
    "            biases[k][i]=biases[k][i]-lr*grad/len(input_vec)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights=[]#define weight and bias list\n",
    "init_weights(weights, network, added_input)\n",
    "biases=[]\n",
    "init_biases(biases, network)\n",
    "input_vec_one_set=[np.array([2,2,2]),np.array([3,3]),np.array([4,4])]#dummy test data set1\n",
    "input_vec_two_set=[np.array([3,4,5]),np.array([7,3]),np.array([9,4])]#dummy test data set2\n",
    "input_vec=[]\n",
    "input_vec.append(input_vec_one_set)\n",
    "input_vec.append(input_vec_two_set)\n",
    "label=np.array([1,1,1])#dummy label\n",
    "#out=feed_forward(input_vec, weights, biases, activation_def)\n",
    "lr=0.001\n",
    "epochs=10\n",
    "for epoch in range(epochs):\n",
    "    grad_descent(input_vec, weights, biases, activation_def, label, lr)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
